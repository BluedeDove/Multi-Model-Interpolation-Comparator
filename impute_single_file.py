# impute_single_file.py

import argparse
import yaml
import pandas as pd
import numpy as np
import torch
import os
from sklearn.preprocessing import StandardScaler

# ä»ä½ çš„é¡¹ç›®ä¸­å¯¼å…¥å¿…è¦çš„æ¨¡å—
from models.pypots_wrappers import PyPOTSWrapper
from models.custom_models.my_lstm_imputer import MyLSTMImputer

def impute_entire_file(args):
    """
    ä½¿ç”¨é¢„è®­ç»ƒçš„æ¨¡å‹å¯¹å•ä¸ªå«æœ‰ç¼ºå¤±å€¼çš„CSVæ–‡ä»¶è¿›è¡Œå®Œæ•´æ’è¡¥ã€‚
    """
    # 1. åŠ è½½é…ç½®æ–‡ä»¶
    print(f"ğŸ”„ Loading configuration from: {args.config}")
    try:
        with open(args.config, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
    except FileNotFoundError:
        print(f"âŒ Error: Config file not found at {args.config}")
        return

    # 2. æ‰¾åˆ°æŒ‡å®šçš„æ¨¡å‹å®šä¹‰
    model_def = next((m for m in config['model_definitions'] if m['name'] == args.model_name), None)
    if not model_def:
        print(f"âŒ Error: Model '{args.model_name}' not found in configuration file.")
        return
    
    # æå–ä¸è¯¥æ¨¡å‹è®­ç»ƒæ—¶ç›¸å…³çš„æ•°æ®å‚æ•° (ä»¥ç¬¬ä¸€ä¸ªtaskä¸ºå‡†)
    # è¿™é‡Œçš„å‡è®¾æ˜¯ï¼Œç”¨äºåŒä¸€æ¨¡å‹çš„n_stepsç­‰å‚æ•°åœ¨ä¸åŒtaskä¸­æ˜¯ä¸€è‡´çš„
    data_config = config['tasks'][0]['data']
    n_steps = data_config['n_steps']
    n_features = data_config['n_features']
    feature_columns = data_config.get('feature_columns')

    # 3. åŠ¨æ€å®ä¾‹åŒ–æ¨¡å‹
    print(f"ğŸ› ï¸ Instantiating model: {args.model_name}")
    device = torch.device(config['global_settings'].get('device', 'cpu'))
    
    # å°†å¿…è¦çš„å‚æ•°æ³¨å…¥åˆ°æ¨¡å‹è¶…å‚æ•°ä¸­
    model_def['hyperparameters']['n_steps'] = n_steps
    model_def['hyperparameters']['n_features'] = n_features
    model_def['hyperparameters']['device'] = device
    
    imputer = None
    if model_def['type'] == 'pypots':
        imputer = PyPOTSWrapper(model_def['class_name'], model_def['hyperparameters'])
    elif model_def['type'] == 'custom':
        # è¿™é‡Œå¯ä»¥æ ¹æ®éœ€è¦æ‰©å±•ï¼Œæ”¯æŒæ›´å¤šçš„è‡ªå®šä¹‰æ¨¡å‹
        if model_def['class_name'] == 'MyLSTMImputer':
            imputer = MyLSTMImputer(**model_def['hyperparameters'])
        # else: ...
    
    if not imputer:
        print(f"âŒ Error: Unknown model type '{model_def['type']}' or class '{model_def['class_name']}'.")
        return

    # 4. åŠ è½½é¢„è®­ç»ƒçš„æ¨¡å‹æƒé‡
    print(f"ğŸ’¾ Loading pre-trained weights from: {args.model_path}")
    if not os.path.exists(args.model_path):
        print(f"âŒ Error: Model file not found at {args.model_path}")
        return
    imputer.load(args.model_path)
    print("âœ… Model loaded successfully.")

    # 5. åŠ è½½å¹¶é¢„å¤„ç†å¾…æ’è¡¥çš„CSVæ–‡ä»¶
    print(f"ğŸ“„ Loading data for imputation from: {args.input_csv}")
    df_missing = pd.read_csv(args.input_csv)
    
    if feature_columns:
        features_df = df_missing[feature_columns].copy()
    else:
        # å¦‚æœconfigä¸­æœªæŒ‡å®šï¼Œåˆ™ä½¿ç”¨æ‰€æœ‰åˆ—ï¼ˆé™¤äº†å¯èƒ½çš„æ—¶é—´åˆ—ï¼‰
        features_df = df_missing.copy()

    # åŒæ ·å¤„ç†å•ç‰¹å¾å¤åˆ¶çš„æƒ…å†µï¼Œä»¥åŒ¹é…æ¨¡å‹è¾“å…¥
    if n_features > 1 and features_df.shape[1] == 1:
        print(f"âš ï¸ Detected single-feature data. Duplicating column to match model's expected input of {n_features} features.")
        original_col_name = features_df.columns[0]
        for i in range(1, n_features):
             features_df[f"{original_col_name}_dup{i}"] = features_df[original_col_name]

    original_data = features_df.values
    
    # æ ‡å‡†åŒ–æ•°æ® (ä¸loader.pyä¸­çš„é€»è¾‘ä¿æŒä¸€è‡´)
    scaler = StandardScaler()
    missing_mask_for_scaling = np.isnan(original_data)
    data_filled_temp = np.nan_to_num(original_data)
    data_scaled = scaler.fit_transform(data_filled_temp)
    data_scaled[missing_mask_for_scaling] = np.nan
    
    # å°†æ•´ä¸ªåºåˆ—åˆ’åˆ†ä¸ºé‡å çš„çª—å£/æ ·æœ¬
    if len(data_scaled) < n_steps:
        print(f"âŒ Error: Data length ({len(data_scaled)}) is smaller than the model's required window size `n_steps` ({n_steps}).")
        return
    
    windows = np.array([data_scaled[i:i+n_steps] for i in range(len(data_scaled) - n_steps + 1)])
    print(f"ğŸ“Š Data preprocessed into {windows.shape[0]} overlapping windows.")

    # 6. æ‰§è¡Œæ’è¡¥
    print("â³ Performing imputation on all windows...")
    imputed_windows_scaled = imputer.impute(windows)
    print("âœ… Imputation complete.")

    # 7. "æ‹¼æ¥"é‡å çš„çª—å£ä»¥é‡å»ºå®Œæ•´åºåˆ—
    # æˆ‘ä»¬é€šè¿‡å¯¹é‡å éƒ¨åˆ†å–å¹³å‡å€¼æ¥å¹³æ»‘åœ°é‡å»ºæ•´ä¸ªåºåˆ—
    print("ğŸ§© Stitching overlapping windows by averaging...")
    reconstructed_data_scaled = np.zeros_like(original_data, dtype=float)
    imputation_counts = np.zeros_like(original_data, dtype=float)

    for i, window in enumerate(imputed_windows_scaled):
        reconstructed_data_scaled[i : i + n_steps] += window
        imputation_counts[i : i + n_steps] += 1
    
    # é¿å…é™¤ä»¥é›¶
    imputation_counts[imputation_counts == 0] = 1
    reconstructed_data_scaled /= imputation_counts
    
    # 8. é€†æ ‡å‡†åŒ–ï¼Œæ¢å¤åˆ°åŸå§‹æ•°æ®å°ºåº¦
    reconstructed_data_orig_scale = scaler.inverse_transform(reconstructed_data_scaled)

    # 9. å°†æ’è¡¥å€¼å¡«å…¥åŸå§‹DataFrameçš„ç¼ºå¤±ä½ç½®
    final_df = df_missing.copy()
    missing_mask_for_filling = final_df[features_df.columns].isnull()
    
    # åˆ›å»ºä¸€ä¸ªä¸ features_df ç›¸åŒåˆ—åçš„ DataFrame ç”¨äºèµ‹å€¼
    imputed_values_df = pd.DataFrame(reconstructed_data_orig_scale, columns=features_df.columns)
    
    final_df[features_df.columns] = final_df[features_df.columns].where(~missing_mask_for_filling, imputed_values_df)

    # 10. ä¿å­˜ç»“æœ
    final_df.to_csv(args.output_csv, index=False)
    print(f"ğŸ‰ Success! Imputed file saved to: {args.output_csv}")


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="Forward-only Imputation Script for a single file.")
    
    parser.add_argument('--config', type=str, required=True,
                        help='Path to the experiment configuration YAML file (e.g., experiment_basic.yaml).')
    
    parser.add_argument('--model-name', type=str, required=True,
                        help='The name of the model to use (e.g., "SAITS", "CSDI", "MyLSTM"). Must match a name in the config.')
                        
    parser.add_argument('--model-path', type=str, required=True,
                        help='Path to the pre-trained model .pth file.')
                        
    parser.add_argument('--input-csv', type=str, required=True,
                        help='Path to the CSV file with missing values that you want to impute.')
                        
    parser.add_argument('--output-csv', type=str, required=True,
                        help='Path where the fully imputed CSV file will be saved.')
                        
    args = parser.parse_args()
    
    impute_entire_file(args)

